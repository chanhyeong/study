= 13. 카프카의 발전과 미래

== 주키퍼 없는 카프카의 미래

=== 제약사항

==== 컨슈머의 오프셋 저장소 전환

* 카프카 2.0 이전에는 znode 를 사용 -> 주키퍼 부담이 높아져서 내부 토픽으로 이전

==== 운영 부담

. 각기 다른 API 사용에 따른 부담
** 주키퍼, 카프카의 개별 운영으로 API 간 호환성, 사용 방벙이 다름 -> 혼동, 개발 생산성 저하
. 운영 여려움
** 추가적인 모니터링 시스템 구성 등
. 보안 문제
** 주키퍼, 카프카 따로 보안 적용 필요
. 메타데이터 관리 문제
** 데이터가 많은 경우 새로운 컨트롤러 선출 시 비용 문제

=== 주키퍼 의존성을 제거한 카프카 업그레이드

메타데이터를 주키퍼 -> 카프카로 이동

* 기존: 컨트롤러 브로커. 나머지 브로커들에 주키퍼에 있는 메타데이터 push
* 신규: 컨트롤러 노드 (리더, 팔로워). 팔로워가 리더에서 pull

==== 업그레이드 방법

한 번에 업그레이드가 아닌 주키퍼 의존성을 완전히 격리시키는 bridge 버전 업그레이드 -> 주키퍼가 없는 정식 버전으로 업그레이드

== KRaft

=== Raft

* 합의 알고리즘, 성능 + 내결함성
* 내부 합의 (여러 서버들의 동의) 를 통한 정보 유지
** 과반수
* follower, candidate, leader
** 처음 시작은 모두 follower
** leader 로 부터 heartbeat 를 받는데, 못받으면 candidate 로 바뀌고 투표 요청
** 과반수 투표 받으면 새로운 leader 로 선정
* etcd, consul 등에서 사용

=== 카프카를 위해 별도로 만들었다ß

* (Kafka 의 선호) <-> (Raft)
** pull <-> push
** offset, epoch <-> index, term
* quorum 상태 메타데이터를 카프카에서 직접 관리
* leader 는 무한정 커지지 않도록 주기적으로 스냅샷 생성
** 다른 컨트롤러들에서 장애 발생 시 전체 데이터 복구하지 않고 스냅샷에서 필요한 부분만 복구
** 뭐 때문에 커지고 뭐를 스냅샷으로 만든다는거지? 메타데이터?

== 최적화된 컨트롤러 노드 구성

컨트롤러 노드 = 브로커와 별개의 JVM 프로세스로 분리되어 실행

=== 동일 서버에서 브로커 + 컨트롤러 실행

* 컨트롤러를 위한 서버가 필요 없으므로 서버 절약

=== 브로커, 컨트롤러 서버 분리

* 서버 장애가 발생한 경우 문제가 되는 서버에서 실행되는 프로세스만 종료. 높은 안정성
* 결국 운영 부담에 있는거랑 비슷한듯?
* 이거에 대한 서버 사양은 어떻게 될지?

== KIP

Kafka Improvement Proposals