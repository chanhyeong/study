= 4. 부호화와 발전

* 하위 호환성: 새로운 코드는 예전 코드가 기록한 데이터를 읽을 수 있음
* 상위 호환성: 예전 코드는 새로운 코드가 기록한 데이터를 읽을 수 있음

상위 호환성이 어렵다

데이터 부호화를 위한 다양한 형식을 볼 예정 = 어떻게 스키마를 변경하고 전/후 공존하는 시스템을 지원하는지

JSON, XML, Protocol Buffers, Thirft, Avro +
REST, RPC, MQ

== 데이터 부호화 형식 (Formats for Encoding Data)

메모리에 object, struct, list, array, hash table, tree 등으로 데이터가 유지

데이터를 다른 곳으로 이동시키려면 byte array 의 형태로 부호화해야 한다 +
byte array 는 메모리에서 사용하는 데이터 구조와는 다르다

메모리 <-> byte array 간 전환이 필요하다

* encoding (serialization or marshalling): inmemory -> byte arry
* decoding (parsing, deserialization, unmarshalling): byte arry -> inmemory

=== 언어별 형식

프로그래밍 언어에 내장된 부호화 라이브러리는 최소한의 추가 코드로 인메모리 객체를 저장하고 복원 +
편리하지만 심각한 문제점도 많다

* 언어에 종속되어 다른 언어에서 읽기 어려움
* 동일한 객체 유형의 데이터를 복원하려면 복호화 과정이 임의의 클래스를 인스턴스화할 수 있어야 한다
** 보안 문제의 원인 - 공격자가 애플리케이션을 얻는 경우 임의의 클래스를 인스턴스화 할 수 있음, 원격으로 임의 코드 실행
* 버전 관리 - 상위/하위 호환성 처리가 미뤄지게 된다
* 효울성 처리도 미뤄짐 (Java 의 내장 직려화를 성능이 좋지 않고 비대해지는 부호화)

=== JSON, XML, Binary variants

* 수 (number) 의 부호화가 애매하다
** XML, CSV 는 number, digit 으로 구성된 문자열을 구분할 수 없다
** JSON 은 문자열과 수를 구분, 정수와 부동소수점 수 구별 X, 정밀도를 지정하지 않음
* JSON, XML 은 유니코드 문자을을 잘 지원하지만, 이진 문자열을 지원하지 않는다
** 이진 데이터를 Base64 를 사용하여 텍스트로 부호화해서 해결 - 데이터 크기 33% 증가
* XML, JSON 모두 스키마 지원 - 익히고 구현하기 난해
* CSV 는 스키마가 없어 의미를 정의하는걸 애플리케이션이 해야 한다.

결점이 있지만 다양한 용도에 사용하기에 충분하므로 앞으로도 인기 있을 것

==== 이진 부호화

큰 데이터셋을 위한 공간 효율을 위해 JSON, XML 용 이진 부호화 개발이 이어짐 +
(BSON, WBXML, ...)

일부는 데이터타입 셋을 확장하지만, JSON/XML 데이터 모델은 유지 +
스키마를 지원하지 않아 부호화된 데이터 안에 모든 객체 필드 이름을 포함해야 함

p119 그림 4-1 은 예제 4-1 을 MessagePack 으로 이진 부호화한 예시

=== Thrift, Protocol Buffers (protobuf)

부호화할 데이터를 위한 스키마가 필요

[code]
----
// thrift
struct Person {
    1: required string          userName,
    2: optional i64             favoriteNumber,
    3: optional list<string>    interests
}

// protobuf
message Person {
    required string user_name       = 1;
    optional int64 favorite_number  = 2;
    repeated string interests       = 3;
}
----

애플리케이션 코드는 생성된 코드를 호출해 스키마르 레코드를 부호화/복호화 할 수 있음

thrift 부호화된 데이터 형식이 2개, BinaryProtocol (그림 4-2), CompactProtocol (그림 4-3)

protobuf 는 CompactProtocol 과 비슷 (그림 4-4)

==== 필드 태그와 스키마 발전

schema evolution: 스키마가 시간이 지남에 따라 변함

부호화된 레코드는 부호화된 필드의 연결

각 필드를 태그 숫자로 식별, 데이터타입을 주석으로 +
스키마에서 필드 이름은 변경할 수 있음, 필드 태그는 변경할 수 없음 +
(부호화된 데이터에서 필드 이름 참조 안함, 필드 태그는 데이터 인식에 필요)

필드에 새로운 태그 변호를 부여하는 방식으로 새로운 필드 추가 (상위 호환성 - 예전 코드에서는 추가된거 무시) +
고유한 태그 번호가 있는 동안에는 계속 같은 의미를 가짐, 새로운 코드는 예전 데이터를 항상 읽을 수 있음 (하위 호환성)

하위 호환성을 유지하려면 추가되는 모든 필드는 required 가 아니라 optional 로 하거나 기본값을 가져야 한다 +
(예전 코드에서는 기록되지 않으므로 깨진다)

필드는 optional 필드만 삭제할 수 있고, 같은 태그 번호는 절대 다시 사용할 수 없다

==== 데이터타입과 스키마 발전

데이터타입 변경은 값이 정확하지 않거나 잘릴 위험

protobuf 의 repeated: 목록, 배열 +
optional (단일 값) -> repeated (다중 값) 로 변경해도 문제가 없다 +
읽는 코드에서 마지막 엘리먼트만 봄

thrfit 의 전용 목록 데이터타입: 목록 엘리먼트의 데이터 타입을 매개변수로 받는다 (패스)

=== Avro

thrift 가 hadoop 의 사용 사례에 적합하지 않아 시작

.두 개의 스키마 언어
[code]
----
// Avro IDL
record Person {
    string          userName;
    union { null, long } favoriteNumber = null;
    array<string>   interests;
}

// JSON
{
    "type": "record",
    "name": "Person",
    "fields": [
        {"name": "userName", "type": "string"},
        ...
    ]
}
----

그림 4-5. 길이 + UTF-8 바이트, ...

binary data 파싱: 스키마에 나타난 순서대로 필드를 보고 데이터타입을 미리 파악 +
(필드나 데이터타입을 식별하기 위한 정보가 없음, 단순 연결된 값)

==== writer's schema, reader's schema

* writer's schema: 외부로 전송 - 알고 있는 스키마 버전을 사용해 데이터 부호화
* reader's schema: 외부로부터 수신 - 특정 스키마로 복호화

writer/reader 가 동일하지 않아도 되며 호환만 가능하면 된다는 개념

Avro 라이브러리가 read 시점에 writer/reader's 스키마를 둘 다 보고 writer's schema 에서 reader's schema 로 데이터를 변환하여 차이 해소

필드 순서가 달라도 schema resolution 에서는 이름으로 필드 일치 시킴 +
reader 에는 없고 writer 에 있으면 무시 +
writer 에는 없고 reader 에 있으면 reader 에 선언된 기본값으로 채움

==== 스키마 발전 규칙

호환성 유지를 위해서 기본값이 있는 필드만 추가 / 삭제 가능

Avro 에서는 임의 변수의 기본값으로 null 을 허용하지 않음 - **union 타입을 사용해야 한다** +
`union { null, long, string } field;` - field 가 수/문자열/null

optional, required 는 없고 union 과 기본값이 있다

데이터타입 변경 가능

==== 쓰기 스키마란?

reader 가 writer's schema 를 어떻게 알 수 있나?

* 대용량 파일
** 일반적인 용도. 파일의 시작 부분에 한 번만 writer's schema 를 포함시키면 된다
** 파일 형식 명시 (object container file)
* DB
** 부호화된 레코드의 시작 부분에 버전 번호 포함, DB 에 스키마 버전 목록 유지
** reader 가 버전 번호를 추출하여 DB 에서 가져옴
** (schema registry 가 이건가)
* 네트워크 연결을 통해 레코드 보내기
** 두 프로세스가 통신할 때 스키마 버전 합의. 연결 유지 동안 합의된 스키마 사용 (Avro RPC)

==== 동적 생성 스키마

스키마에 태그 번호가 없어 동적 생성 스키마에 더 친숙하다

관계형 스키마에서 Avro 스키마를 쉽게 생성할 수 있다 +
Avro 스키마 -> DB 내용 부호화 -> Avro object container file 형식으로 덤프

DB 스키마가 변경되면? +
갱신된 DB 스키마 -> 새로운 Avro 스키마 -> ... +
읽는 시점에서 필드 변경 인지 - 필드는 이름으로 식별되어 writer 는 여전히 reader 와 매치 가능

==== 코드 생성과 동적 타입 언어

Avro 는 정적 타입 언어를 위해 코드 생성을 선택적으로 제공, 코드 생성 없이도 사용 가능 +
object container file 이 있다면 Avro 라이브러리로 데이터를 볼 수 있음

=== 스키마의 장점

이진 부호화를 독자적으로 구현하기도 한다 +
ex) RDB 의 네트워크 프로토콜 - 특정 DB 에 특화되고 DB 벤더가 드라이버 제공

이점들

* 부호화된 데이터에서 필드명 생략 가능, binary JSON 보다 크기가 작을 수 있다
* 유용한 문서화 형식. 복호화할 때마다 필요하기 때문에 스키마가 최신 상태인지 확신할 수 있다
* 스키마 DB 를 유지하면 변경이 적용되기 전 상위/하위 호환성 확인 가능
* 정적 타입 언어에 스키마로부터 코드 생성 기능이 유용하다. 컴파일 시점에 타입 체크 가능

schemaless or schema-on-read JSON DB 가 제공하는 것과 동일한 종류의 유연성 제공 +
데이터나 도구 지원도 더 잘 보장

== DataFlow 모드

하나의 프로세스에서 다른 프로세스로 데이터를 전달하는 방법들

* DB 를 통해서
* 서비스 호출을 통해서
* 비동기 메시지 전달을 통해서

=== DB 를 통해서

기록 - 부호화, 읽기 - 복호화

그림 4-7. 데이터 유실 가능성

==== 다양한 시점에 기록된 다양한 값

데이터가 코드보다 더 오래 산다. data outlives code

전체를 새로운 스키마로 rewriting 하기보단 +
null 기본값을 갖는 새로운 컬럼을 추가하는 간단한 스키마 변경

(MySQL 에서 ALTER 날리면 전체 테이블 복사 후 생성? 지금도?, https://myinfrabox.tistory.com/61[Online DDL 에서는 아닌듯])

여러 버전의 스키마로 부호화된 레코드를 포함하더라도 (컬럼이 추가되기 이전 데이터들도) +
단일 스키마로 부호화된 것처럼 보이게 한다

==== 보관 저장소

백업, data warehouse 로 적재하기 위해 DB 스냅샷을 수시로 만들 때 +
최신 스키마를 사용해 부호화

덤프는 한 번에 기록하고 변하지 않으므로 Avro object container file 같은 형식이 적합하다

=== 서비스를 통해서: REST, RPC

서비스: 서버가 공개한 API

service-oriented architecture, SOA: 하나의 서비스가 다른 서비스의 일부 기능이나 데이터가 필요하다면, 해당 서비스에 요청을 보냄 +
현재에 와서는 microservice architecture, MSA

SOA, MSA 의 핵심 설계 목표: 서비스를 배포와 변경에 독립적으로 만들어 변경/유지보수를 쉽게

==== 웹 서비스

기본 프로토콜로 HTTP 사용

===== REST

프로토콜이 아닌 설계 철학

데이터 타입 강조, URL 로 리소스 식별 +
캐시 제어, 인증, 컨텐츠 타입 협상에 HTTP 기능 사용

===== SOAP

네트워크 API 요청을 위한 XML 기반 프로토콜

HTTP 와 독립적이며 대부분의 HTTP 기능을 사용하지 않음 +
복잡한 여러 관련 표준 제공 - `WS-*` 웹 서비스 프레임워크

API 를 WSDL 이라는 XML 기반 언어를 사용해 기술 +
WSDL 은 사람이 읽을 수 없고, 메시지를 수동으로 구성하기에는 복잡하다

==== RPC 문제

EJB, RMI, DCOM, CORBA

RPC: 원격 네트워크 서비스 요청을 함수나 메소드를 호출하는 것과 동일하게 사용하게 해줌 +
(위치 투명성, location transparency)

로컬 함수 호출은 예측 가능, 네트워크 요청은 예측이 어려움 +
네트워크 문제로 요청/응답 유실 or 원격 장비 느려짐 or 요청에 응답하지 않을 수 있다

* 네트워크 요청은 타임아웃이 날 수 있다
* 실패한 네트워크 요청을 다시 시도할 때 실제로는 처리되고 응답만 유실될 수 있다
* 느리고, 지연 시간이 매우 다양하다
* 데이터 전송 시 byte array 로 부호화해야 한다
* 하나의 언어에서 다른 언어로 데이터타입을 변환해야 한다
* (gRPC 는 위와 같은 문제를 어떻게 하고 있는 걸까)

==== RPC 의 현재 방향

차세대 RPC 프레임워크는 원격 요청이 로컬 함수 호출과 다르다는 것을 분명히 한다

Finagle, Rest.li: 실패 가능성이 있는 비동기 작업 캡슐화 - future
gRPC: 하나의 요청과 하나의 응답 + 시간에 따른 일련의 요청과 응답으로 구성된 스트림 지원

binary 로 주고 받는 RPC 프로토콜이 성능이 더 나을 수도 +
(REST 는 생태계가 다양하다는 이점)

RPC 는 같은 데이터센터 내의 같은 조직이 소유한 서비스 간 요청

==== 데이터 부호화와 RPC 의 발전

클라이언트와 서버를 독립적으로 변경하고 배포할 수 있어야 한다

모든 서버를 먼저 갱신 (하위 호환성) -> 클라이언트가 갱신 (상위 호환성)

RPC 스키마는 이전 특성과 같음

=== 메시지 전달을 통해

비동기 메시지 전달 시스템, 단방향

직접 네트워크가 아닌 임시로 메시지를 저장하는 메시지 브로커 or 메시지 지향 미들웨어 를 거쳐 전송

* 수신자에 문제가 있더라도 브로커가 버퍼처럼 동작 - 시스템 안정성 향상
* 죽었던 프로세스에 메시지 다시 전달 가능 - 메시지 유실 방지
* 송신자가 수신자 정보를 알 필요가 없다
* 하나의 메시지를 여러 수신자로 전송 가능
* 논리적인 송/수신자 분리

==== 메시지 브로커

RabbitMQ, ActiveMQ, HornetQ, NATS, Apache Kafka

단방향 데이터플로만 제공 +
Request-Reply 할 수도 있음

특정 데이터 모델을 강요하지 않는다

==== 분산 액터 프레임워크

actor model: 단일 프로세스 안에서 동시성을 위한 프로그래밍 모델

* thread 를 직접 처리하는 대신 로직이 actor 에 캡슐화
* actor: 하나의 클라이언트나 엔티티
** 로컬 상태를 가질 수 있고 비동기 메시지 송수신으로 다른 actor 와 통신
** 메시지 전달 보장하지 않음. 유실될 수 있음
** 한 번에 하나의 메시지만 처리

분산 액터 프레임워크 - 여러 노드 간 애플리케이션 확장에 사용

송/수신자가 어디 위치하는지 관계 없이 동일한 메시지 전달 구조 사용

메시지 브로커와 actor model 을 단일 프레임워크에 통합 +
롤링 업그레이드 시에는 상하위 호환성에 주의해야 한다

Akka, Orleans, erlang