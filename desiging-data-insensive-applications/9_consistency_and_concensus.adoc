= 9. 일관성과 합의

tolerant 시스템에 쓰이는 알고리즘과 프로토콜을 볼 예정

합의 - 모든 노드가 어떤 것에 동의하게 만드는 것

== 일관성 보장

최종적 일관성은 매우 약한 보장이다 - 언제 복제본이 수렴될지 모름

시스템이 선택적으로 제공할 수 있는 강한 일관성 모델들이 뒤에 나온다 +
성능이 나쁘거나 tolerance 가 약할 수도 있으나 올바르게 사용하기 쉽다

분산 일관성은 지연, 결함이 있더라도 복제본의 상태를 코디네이션하는 것에 관함

선형성 (linearizability), 이벤트 순서화 문제, 분산 트랜잭션을 원자적으로 커밋하는 방법 등

== 선형성

복제본이 하나만 있다는 환상을 만들어준다면? a.k.a. atomic / strong / immediate / external consistency +
복사본이 하나만 있고 해당 데이터를 수행하는 모든 연산을 원자적으로 보이게

읽힌 값이 최근에 갱신된 값이며, 오래된 캐시나 복제본에서 나온 값이 아님을 보장 - recency guarantee

그림 9-1 비선형성

=== 무엇이 선형성을 부여?

==== 그림 9-3

한 클라이언트가 바뀐 값을 반환하면, 이후 모든 읽기도 바뀐 값을 반환 (쓰기 연산이 완료되지 않았더라도)

==== 그림 9-4

연산이 실행됐다고 생각하는 시점이 수직선으로 표시, 모여서 순차열

연산 표시를 모은 선들이 항상 시간순으로 진행되어야 하고, 뒤로 가서는 안된다 -> 최신성 보장

* 실행 시점은 (네트워크 지연을 고려한) db 인입 시점
* 트랜잭션 격리를 가정하지 않음, 과거 값을 읽는 것을 허용하지 ㅇ낳음

.선형성 vs 직렬성
****

* 직렬성: 트랜잭션들의 격리 속성, 특정 순서에 따른 트랜잭션들의 순서 보장
* 선형성: R/W 에 대한 최신성 보장, 연산을 트랜잭션으로 묶지 않음

strict serializability, strong one-copy serializability: 직렬성과 선형성을 둘 다 제공

****

=== 선형성에 기대기

==== 잠금과 리더 선출

모든 노드가 lock 획득, 성공한 노드가 리더

zookeeper, etcd 등의 코디네이션 서비스 사용 - 합의 알고리즘 사용하여 선형성 연상을 tolerant 하게 +
curator - zookeeper 위에 고수준 레시피 제공

분산 잠금 예로는 Oracle RAC - 여러 노드가 동일한 디스크 저장 시스템 공유, 디스크 페이지마다 잠금

==== 제약 조건과 유일성 보장

==== 채널 간 타이밍 의존성

=== 선형성 시스템 구현

"데이터 복사본이 하나만 있는 것처럼 동작하고 그 데이터에 실행되는 모든 연산은 원자적"

복제 방법에 따른 확인 (p331)

==== 선형성과 정족수

정족수가 만족되어도 최신 값을 보지 못하는 경우 (그림 9-6)

읽기 복구를 동기식으로 하면 해결되지만 느림

다이나모 스타일 복제를 하는 리더 없는 시스템은 선형성을 제공하지 않는다고 보는게 안전하다

=== 선형성의 비용

다중 리더에서는 네트워크가 끊겨도 정상 동작할 수 있다 +
(근데 끊겨있는 동안은 비선형적일 수 있는거 아닌가? - 클라이언트가 데이터센터마다 붙어있어서 아니라고 보는 것 같기도)

단일 리더 복제에서 팔로워로 온 R/W 요청은 동기식으로 리더로 보내져야 한다 +
네트워크가 끊기면 리더로 연결할 수 없다 - 팔로워에서 읽을 때 비선형적일 수도

==== 선형성과 네트워크 지연

선형성을 제거한 이유는 성능

지연이 심한 네트워크에서 선형성 R/W 의 응답 시간은 높아진다

12장에서는 정확성을 희생하지 않고 선형성을 회피하는 방법 설명

== 순서화 보장

연산과 그 연산의 실행 순서를 결합하여 나타냄

=== 순서화와 인과성 (Casuality)

순서화가 인과성 보존에 도움

인과성은 이벤트에 순서 부과 +
시스템이 그 순서를 지키면 인과적으로 일관적 (casually consistent)

==== 인과적 순서가 전체 (total) 순서는 아니다

전체 순서, 부분 순서 (비교 불가)

* 선형성: 연산의 전체 순서를 정할 수 있음
* 인과성: 두 이벤트가 동시에 실행되면 비교할 수 없다. 부분 순서를 정의

인과적 일관성 그래프와 유사한 예) git version history

==== 선형성은 인과적 일관성보다 강하다

선형성은 인과성을 내포. 선형적이라면 인과성도 올바르게 유지한다

선형성은 인과성을 보장하는 유일한 방법은 아니다 +
인과적 일관성은 더 효율적으로 구현될 수 있다 - 최종적 일관성 등으로 연구가 되고 있지만 아직 연구 진행 중

==== 인과적 의존성 담기

연산이 먼저 실행됐는지 알아야 한다 = 후속 연산은 선행 연산 처리를 기다려야 한다

버전 벡터, SSI 충돌 검출 등

=== 일련변호 순서화

일련번호, 타임스탬프로 이벤트의 순서 정의 - 전체 순서

인과성에 일관적인 전체 순서대로 일련번호를 생성할 수 있다

단일 리더에서는 쉽다

==== 비인과적 일련번호 생성기

단일 리더가 없다면,

* 각 노드마다 독립적인 일련번호 집합 생성 (ex. 홀/짝)
* 물리적 시계에서 얻은 타임스탬프를 붙이기
** 해상도가 높다면 전체 순서를 정하는데도 사용할 수 있음. LWW 에서 사용
* 일련번호 블록을 미리 할당 (1~1000, 1001~2000)

일련번호가 인과성에 일관적이지 않다

==== 램포드 타임스탬프

인과성에 일련번호를 생성하는 간단한 방법

(counter, Node ID)

각 노드가 고유 식별자를 갖고, 처리한 연산 개수를 카운터로 유지 +
카운터가 큰게 타임스탬프가 크다. 카운터가 같으면 노드 id 가 큰게 타임스탬프가 크다

==== 타임스탬프 순서화로는 충분하지 않다

유일성 제약 조건 등을 구현하려면 전체 순서로는 충분하지 않다 +
언제 그 순서가 확정되는지 알아야 한다

=== 전체 순서 브로드캐스트

노드 사이에 메시지를 교환하는 포로토콜로 기술. 비공식적인 두 가지 안정성 속성을 만족해야 한다

* 신뢰성 있는 전달 (reliability delivery)
* 전체 순서가 정해진 전달 (totally ordered delivery)

노드나 네트워크 결함이 있더라도 항상 만족되도록 보장해야 한다

==== 사용하기

zookeeper, etcd 등에서 구현되어 있음 - 전체 순서 브로드캐스트와 합의는 간한 연관이 있다

모든 복제 서버가 연산을 같은 순서로 처리하면 일관성 있는 상태로 유지 - state machine replication

메시지가 전달되는 시점에 그 순서가 고정된다 - 앞으로 끼워 넣는걸 불허

==== 사용하여 선형성 저장소 구현하기

전체 순서 브로드캐스트는 비동기식 - 메시지 순서는 보장되나 언제 메시지가 전달될지는 보장되지 않음

전체 순서 브로드캐스트를 추가 전용 로그로 사용해 선형성 compare-and-set 연산 구현

. 메시지를 로그에 추가, 점유하기 원하는 사용자명을 시험적으로 가리킴
. 로그를 읽고, 추가한 메시지가 되돌아오기를 기다림
. 사용자명을 점유하려고 하는 메시지가 있는지 확인
** 첫 번째 메시지가 다른 사용자가 보낸거라면 abort

?

==== 선형성 저장소를 사용하여 전체 순서 브로드캐스트 구현하기

모든 메시지에 선형성 정수 일련번호를 붙인다 +
수신자들은 일련번호 순서대로 메시지를 전달. 처리 순서를 알 수 있음

== 분산 트랜잭션과 합의

합의가 중요한 상황들: 리더 선출, 원자적 커밋 등

=== 원자적 커밋과 2PC

==== 단일 노드에서 분산 원자적 커밋으로

단일 노드에서 트랜잭션 커밋은 지속성 있게 쓰여지는 순서에 의존 +
커밋을 원자적으로 만들어두는게 단일 장치 (디스크 드라이브의 컨트롤러)

트랜잭션에 여러 노드가 관여한다면? +
어떤 노드에선 커밋, 어떤 노드에서 실패하면 일관성이 없어진다

트랜잭션 커밋은 되돌릴 수 없어야 한다

==== 2PC 소개

모든 노드가 commit/abort 되도록 보장하는 알고리즘

coordinator (= transaction manager) 사용 +
애플리케이션 프로세스 내 라이브러리 or 분리된 프로세스나 서비스 등으로 존재

애플리케이션이 데이터를 read/write 하는데 포함되는 여러 db 노드: participant

* 1단계: 각 노드에 준비 요청
** 모든 참여자가 yes 응답이면 2단계에서 커밋
** 하나라도 no 면 어보트

==== 약속에 관한 시스템

2PC 를 다르게 만들어주는 것

. 애플리케이션: 분산 트랜잭션을 시작할 때 코디네이터에게 TxID 요청 (global unique)
. 애플리케이션: 각 참여자에서 단일 노드 트랜잭션 시작, 유일한 TxID 부여
. 애플리케이션: 모든 참여자에 TxID 로 준비 요청 보냄
. 참여자: 준비 요청을 받으면 트랜잭션 커밋이 가능한지 확인
. 코디네이터: 모든 준비 요청에 대한 응답을 받았을 때 commit/abort 판단
** 결정을 Tx 로그에 기록해야 한다 - 커밋 포인트
. 결정이 쓰여지만 모든 참여자에게 commit/abort 요청 전송
** 요청이 실패하거나 타임아잇아 되면 코디네이터가 성공할 때까지 영원히 재시도

==== 코디네이터 장애

참여자는 코디네이터의 회신을 기다려야 한다 +
이 상태에 있는 참여자의 트랜잭션 - in doubt or uncertain

2PC 가 완료되는 유일한 방법은 코디네이터가 복구되길 기다리는 것 뿐

코디네이터가 복구될 때 코디네이터의 로그에 커밋 레코드가 없는 트랜잭션 어보트

(아마 내부도 이래서 multi 지원을 안함)

==== 3PC 커밋

2PC 는 코디네이터 복구 대기로 인해 블로킹 원자적 커밋 프로토콜

3PC 는 지연에 제한이 있는 네트워크, 응답 시간에 제한이 있는 노드를 가정 +
https://medium.com/curg/%EC%83%A4%EB%94%A9-%ED%94%84%EB%A1%9C%ED%86%A0%EC%BD%9C-part3-atomic-properties-f91d3030ed67[ready 와 commit 사이에 pre-commit 을 넣는다]

논블로킹 원자적 커밋 - perfect failure detector, 노드가 죽었는지 아닌지 구별할 수 있는 신뢰성 있는 매커니즘이 필요 +
기약 없는 지연이 있는 네트워크에서 아무도 안 죽었지만 네트워크 문제로 요청이 타임아웃이 될 수 있다

=== 현실의 분산 트랜잭션

운영상 문제로 분산 트랜젹션 구현을 하지 않는 선택을 하기도

분산 트랜잭션이 무엇을 의미하는가, 두 가지

* DB 내부 분산 트랜잭션
* 이종 (heterogeneous) 분산 트랜잭션

==== 정확히 한 번 메시지 처리

메시지 확인과 DB write 를 단일 트랜잭션에서 원자적 커밋으로 처리하여 구현 +
분산 tx 가 지원되면 가능

메시지와 처리 결과들을 원자적 커밋 - 결과적으로 정확히 한 번 (effectively exactly once) 처리

모든 시스템이 동일한 원자적 커밋 프로토콜을 사용할 수 있을 때만 가능

11장에서 다시 볼 예정

==== XA 트랜잭션

X/Open XA (eXtended Architecture) - 이종 기술에 걸친 2PC 커밋을 구현하는 표준

트랜잭션 코디네이터와 연결되는 인터페이스를 제공하는 C API +
자바는 JTA, JDBC JMS API 등

애플리케이션이 네트워크 드라이버나 클라이언트 라이브러리를 사용해 참여자 (DB/Message) 와 통신 +
드라이버가 XA 를 지원 = 연산이 XA API 를 호출한다 & read, commit, abort 요청할 수 있는 콜백도 제공

트랜잭션 코디네이터는 XA API 구현 +
트랜잭션 참여자 추적, read 요청 보냄, 응답 수집, commit/abort 결정 추적을 위한 로컬 디스크 로그 사용

죽으면 사라진다 - 코디네이터 로그는 로컬 디스크에 있으면 재시작하여 commit/abort 결과를 복구해야 한다 +
(컨테이너에선 어떻게?)

==== in doubt 동안 lock 유지 문제

lock 유지 동안 잡힌 row 를 변경할 수 없음

==== 코디네이터 장애에서 복구하기

현실에서는 고아가 된 트랜잭션이 생길 수 있다 +
2PC 의 올바른 구현은 재시작하더라도 lock 을 유지해야 한다 (원자성 보장)

관리자가 수동으로 트랜잭션 commit/abort 결정하는 방법 뿐

XA 구현에서는 참여자가 코디네이터로부터의 응답 없이 abort/commit 결정을 할 수 있도록 하는 경험적 결정 (heuristic decision) 이 있다 +
== 원자성을 깰 수 있다. 평상시가 아닌 큰 장애 상황을 벗어나고자 할 때만 쓰도록 의도

==== 분산 트랜잭션의 제약

트랜잭션 코디네이터 자체가 트랜잭션 결과를 저장할 수 있는 일종의 DB 여야 한다

* 코디네이터가 복제되지 않고 단일 장비 - SPOF
* 보통 서버 애플리케이션은 모든 상태를 DB 에 저장, 상태 비저장
** 코디네이터가 애플리케이션 서버의 일부가 되면 배포의 특성이 바뀌게 된다 - 로그가 지속적인 시스템 상태의 중대한 부분 -> 상태 비저장이 아님
* XA 는 여러 시스템과 호환, 최소한의 공통 기능 - 여러 시스템에 걸친 deadlock 감지는 불가능. SSI 와 함께 동작하지 않음
* 분산 트랜잭션은 장애를 증폭시키는 경항 - 내결함성 시스템 구축 목적에 어긋난다

=== 내결함성을 지닌 합의

노드들이 값을 제안, 합의 알고리즘의 그 값 중 하나를 결정

합의 알고리즘이 만족해야 하는 속성들

* 균일한 동의: 다르게 결정하지 않음
* 무결성: 두 번 결정하지 않음
* 유효성: 한 노드가 값을 결정하면, 그 값은 어떤 노드에서 제안된 것
* 종료: 죽지 않은 모든 노드는 결국 어떤 값을 결정

==== 합의 알고리즘과 전체 순서 브로드캐스트

내결함성을 지닌 합의 알고리즘 - Viewstamped Replication (VSR), Paxos, Raft, Zab

이 알고리즘들은 위의 속성들을 직접 사용하지 않고 (만족은 한다) +
값의 순차열 (sequence) 에 대해 결정해서 전체 순서 브로드캐스트 알고리즘을 만든다

전체 순서 브르드캐스트: 정확히 한 번, 같은 순서로 전달 - 합의를 여러 번 반복하는 것과 동일

==== 단일 리더 복제와 합의

단일 리더 복제: 복제본이 최신 상태를 유지하게 = 전체 순서 브로드캐스트 +
리더가 이미 수동으로 선택된 상태

리더를 새로 선출하려면 합의가 필요하다 +
합의 알고리즘들이 실제로는 전체 순서 브로드캐스트 = 단일 리더 복제 = 단일 리더 복제는 리더가 필요 -> 리더를 선출하려면 리더가 필요?

==== 에포크 번호 붙이기와 정족수

리더 선출 시 에포크 번호 증가 - 에포크 번호는 전체 순서가 있고 단조 증가 +
두 가지 다른 에포크에 있는 두 가지 다른 리더 사이에 충돌이 있으면, 에포크 번호가 높은 리더가 이긴다

대신 노드의 정족수로부터 투표를 받아야 한다 +
리더는 모든 결정에 대해 제안된 값을 다른 노드에 전송, 정족수 찬성 응답 +
에포크 번호가 더 높은 다른 리더를 알지 못할 때만 제안에 찬성하는 투표

2가지 투표: 리더 선출 투표, 리더의 제안에 투표

==== 합의의 제약

투표하는 과정 - 동기식 복제

엄격한 과반수 동작 요구 - 그만큼의 장비 필요

합의 알고리즘들은 투표에 참여하는 노드 집합이 고정되어 있다고 가정, 노드를 그냥 추가하거나 제거할 수 없음

장애 노드 감지를 타임아웃에 의존 - 일시적 네트워크 문제로 장애 발생으로 오해

Raft: 전체 네트워크가 올바르게 동작하지만, 네트워크 링크 하나가 계속 불안정하면 리더십이 두 노드 사이에서 지속적으로 왔다갔다 or 꾸준히 리더에서 강제로 내려옴

=== 멤버십과 코디네이션 서비스

zookeeper, etcd 는 적은 양의 데이터를 보관하도록 설계 +
소량의 데이터가 전체 순서 브로드캐스트 알고리즘으로 모든 노드에 복제

zookeeper 는 다음 기능들도 구현

* 선형적 원자적 연산
** compare-and-set 으로 lock 구현. 여러 노드가 동시에 같은 연산을 수행하려고 하면 하나만 성공
* 연산의 전체 순서화
** lock 을 획득할 때마다 펜싱 토큰인 zxid (TxID), cversion (버전 번호) 할당
* 장애 감지
** 클라이언트와 heartbeat 교환 - 연결이 일시적으로 끊기더라도 세션은 살아있음
** 세션 타임아웃보다 긴 기간 동안 하트비트가 멈추면 세션이 죽었다고 판단
* 변경 알림
** 값이 변경된걸 감시 - 두기적으로 폴링하는 필요를 피함

==== 작업을 노드에 할당하기

여러 개의 프로세스가 있고 하나가 리더로 선택되어아 할 때

파티셔닝된 자원이 있고 어떤 파티션을 어느 노드에 할당해야 할지 결정해야 하는 경우

매우 많은 노드보단 고정된 노드의 주키퍼에서 과반수 투표를 수행

주기퍼로 관리되는 데이터는 느리게 변한다 +
애플리케이션의 런타임 상태 저장용으로 의도된게 아니다 +
애플리케이션 상태를 복제해야 한다면 Apache BookKeeper 를 사용할 수 있다

==== service discovery

특정 서비스에 연결하려면 어떤 IP 주소로 접속해야 하는지 알아내는 용도

합의가 필요 없음

==== 멤버십 서비스

클러스터에서 어떤 노드가 살아 있는 멤버인지 결정

실제로는 살아 있지만 합의에 의해 죽은것으로 선언될 수도 있긴 하다