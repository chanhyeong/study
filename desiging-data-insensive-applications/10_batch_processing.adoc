= 10. 일괄 처리

:toc:

(파트3 파생 데이터 내용)

데이터 모델이 다르고 접근 양식이 다른 데이터 시스템을 일관성 있는 하나의 애플리케이션 아키텍처로 통합하는 문제 검토

레코드 시스템 (source of truth) - 믿을 수 있는 데이터 버전 저장 +
파생 데이터 시스템 - 다른 시스템의 데이터를 가져와서 변환하고 처리한 결과. 데이터를 잃어도 원천에서 다시 생성할 수 있음

---

온라인 시스템 - 응답 시간 단축에 노력

* 서비스 (온라인 시스템): request - response
* 일괄 처리 시스템 (오프라인 시스템): 매우 큰 입력 데이터 -> 처리 작업 수행하여 결과 데이터 생산
** 수 분 ~ 수 일, 처리량이 주요 성능 지표
* 스트림 처리 시스템 (준실시간 시스템): 요청에 대해 응답하지 않음음. 입력 데이터 소비 - 출력 데이터 생산

== 유닉스 도구로 일괄 처리하기

* 정렬 vs 인메모리 집계

작업 세트가 작으면 인메모리 해시 테이블도 잘 동작 +
작업 세트가 크다면 정렬 접근법을 사용 +
(sort 는 메모리보다 큰 데이터 셋을 자동으로 디스크로 보내고, CPU 코어에서 정렬)

=== 유닉스 철학

유닉스 파이프 - 다른 방법으로 데이터 처리가 필요할 때 여러 다른 프로그램을 연결하는 방법이 필요하다 +
배관 공사와 비슷하여 파이프로 명명

. 각 프로그램이 한 가지 일만 하도록 작성
. 모든 프로그램의 출력이 다른 프로그램의 입력으로 쓰일 수 있다고 생각
. 소프트웨어를 빠르게 써볼 수 있게 설계, 구축
. 작업을 줄이려면 미숙한 도움보단 도구를 사용

==== 동일 인터페이스

프로그램들 간의 연결이 가능하게 하려면 모두가 같은 입출력 인터페이스를 사용해야 한다

유닉스에서는 파일 (파일 디스크립터)

==== 로직과 연결의 분리

파이프는 한 프로세스의 stdout 을 다른 프로세스의 stdin 과 연결 - 인메모리 버퍼 사용

프로그램은 입력이 어디서부터 들어오는지 출력이 어디로 나가는지 신경쓰거나 알 필요가 없음 -> 입출력 연결이 분리되어 작은 도구로 큰 시스템 구성이 훨씬 수월 +
(loose coupling, late binding, inversion of control)

제약사항: 프로그램이 여러 입출력을 처리하기가 까다로움, 네트워크 연결하지 못함 (단일 장비)

==== 투명성과 실험

입력이 불변, 어느 시점이든 중단 가능하여 출력 확인, 특정 단계의 출력을 파일에 기록하고 다음 입력으로 사용할 수 있음

== 맵리듀스와 분산 파일 시스템

유닉스 도구와 비슷하지만 다수의 장비로 분산해서 실행 가능 +
입력 수정 없이 출력 생산 +
분산 파일 시스템 상의 파일을 입출력으로 사용

HDFS - 비공유 원칙 기반. 개발 장비의 데몬 프로세스에서 네트워크 서비스 제공 +
NameNode: 특정 파일 블록이 어떤 장비에 저장됐는지 추적하는 중앙 서버

=== 맵리듀스 작업 실행하기

맵리듀스: 분산 파일 시스템 위에서 대용량 데이터셋을 처리하는 코드를 작성하는 프로그래밍 프레임워크

. 읽고 레코드로 쪼갬 (구분자 \n)
. 레코드마다 매퍼 함수 호출, key-value 추출
. key 를 기준으로 정렬
. key-value 페어 전체를 대상으로 리듀스 함수 호출

Mapper: 입력 레코드에서 key, value 를 추출하는 작업 (데이터 준비) +
Reducer: key-value 페어를 받아 같은 key 를 가진 레코드를 모으고 값의 집합을 반복해 Reducer 함수 호출. 출력 레코드 생산 (데이터 가공)

==== 맵리듀스의 분산 실행

그림 10-1 +
병렬 실행 - 파티셔닝을 기반 +
입력 = 디렉토리, 디렉토리 내 각 파일 or 파일 블록을 독립 파티션으로 간주

복제본이 있는 장비에 리소스 여유가 있으면 스케줄러가 입력 파일이 있는 장비에서 작업을 수행하려고 한다 = **데이터 가까이에서 연산하기** (Put the computation near the data)

애플리케이션 코드는 작업 수행 전 복사된다

리듀서 쪽 연산도 파티셔닝 +
리듀서 태스트 수는 사용자가 설정 -> 맵, 리듀스 태스크 수는 다를 수 있다

셔플: 리듀서를 기준으로 파티셔닝, 정렬. 매퍼로부터 데이터 파티션을 복사하는 과정

리듀스 시 매퍼로부터 파일을 가져와서 정렬된 순서를 유지하며 병합

==== 맵리듀스 워크플로

맵리듀스 작업을 연결하기

하둡 맵리듀스 작업 간 수행 의존성을 관리하기 위한 스케줄러들 - Oozie, Azkaban, Luigi, Airflow, Pinball

다중 맵리듀스를 적절하게 자동으로 엮어 워크플로 설정하는 고수준 도구 - Pig, Hive, Cascading, Crunch, FlumeJava

=== 리듀스 사이드 조인과 그룹화

foreign key, document reference, edge

맵리듀스에서는 일반적인 인덱스 개념이 없다. full table scan

==== 사용자 활동 이벤트 분석 예제

일괄 처리 처리량을 위해서는 한 장비 내에서 연산을 수행해야 한다

==== 정렬 병합 (sort-merge) 조인

그림 10-3. 사용자 DB 와 활동 DB 에서 각각 key-value 추출

리듀서가 로컬 변수에 생년월일 저장 -> 같은 사용자 id 이벤트 순회

매퍼 출력이 키로 정렬 -> 리듀서가 약측의 정렬된 레코드 목록을 병합

(중간에 비어있는 키들에 대해서는 어떻게?)

==== 같은 곳으로 연관된 데이터 가져오기

매퍼와 정렬 프로세스는 특정 key 로 조인 연산을 할 때 필요한 모든 데이터를 한 곳으로 모은다 = key 별로 리듀서를 한 번만 호출

필요한 데이터를 정렬해뒀기 때문에 리듀서는 단일 스레드로 동작하는 코드 조각이 될 수 있다

장비로 데이터를 모으는 연산 (물리적 네트워크 연산) 과 데이터를 처리하는 애플리케이션 로직을 분리

==== 그룹화

단순: key-value 를 생성할 때 키로 그룹화

특정 사용자의 일련의 활동을 찾기 위해 세션별 활동 이벤트 수집 - 세션화

==== 쏠림 (hotspot) 다루기

linchpin object, hot key

* Pig - skewed join
** 어떤게 hot key 인지 결정하기 위해 샘플링
** 실제 조인 시 hot key 를 가진 레코드는 여러 리듀서 중에 임의의 하나로 보냄
** hot key 로 조인할 다른 입력은 모든 리듀서에 복제 - 복제 비용이 크지만 병렬화 효과가 크다
* Crunch - shared join
** 샘플링 작업 대신 hot key 를 명시적으로 지정
* Hive - map-side join
** 테이블 메타데이터에 명시적으로 지정, hot key 와 관련된 레코드를 별도 파일에 저장

그룹화 -> 집계

. 레코드를 임의의 리듀서로 보내서 hot key 레코드의 일부 그룹화 -> key 별로 집계하여 출력
. 위 단계에서 나온 값을 key 별로 모두 결합하여 하나의 값으로 만듦

=== 맵 사이드 조인

이전은 조인 로직을 리듀서에서 수행

데이터에 대한 특정 가정이 필요 없는 장점 +
정렬 후 복사, 병합 비용이 큰게 단점

데이터에 대해 특정이 가능하다면 맵사이드 조인으로 더 빠르게 수행할 수 있다

매퍼는 파일을 읽어 출력하는게 전부

==== 브로드캐스트 해시 조인

작은 데이터셋과 매우 큰 데이터셋을 조인하는 경우

데이터를 읽어서 인메모리 해시에 넣고 -> 다음 데이터를 스캔할 때 해시 테이블에서 간단하게 조회

큰 input 의 파티션 하나를 담당하는 각 매퍼는 작은 input 전체를 읽는다

==== 파티션 해시 조인

조인의 input 을 파티셔닝하고 해시 조인 접근법을 각 파티션에 독립적으로 적용

조인할 레코드들이 같은 파티션에 위치 +
각 매퍼의 해시 테이블에 적재해야 할 데이터 양을 줄일 수 있는게 장점

==== 맵 사이드 병합 조인

데이터셋이 같은 방식으로 파티셔닝 & 같은 key 를 기준으로 정렬됐다면

오름차순으로 양쪽 입력 모두를 점진적으로 읽어 키가 동일한 레코드를 맞춘다 +
(= 선행 맵리듀스 작업이 이미 파티셔닝 & 정렬해두었다)

==== 맵 사이드 조인을 사용하는 맵리듀스 워크플로

맵 사이드 조인 시 크기, 정렬, 입력 데이터의 파티셔닝 등의 제약 조건 +
조인 전략 최적화 시 분산 파일 시스템 내 저장된 데이터셋의 물리적 레이아웃 파악이 중요하다

파티션 수가 몇 개, 데이터가 어떤 키를 기준으로 파티셔닝 & 정렬

=== 일괄 처리 워크플로우 출력

==== 검색 인덱스 구축

전체 문서 집합을 주기적으로 색인 등

==== key-value 저장

==== 일괄 처리 출력에 관한 철학

유닉스와 동일 - 입력을 불변으로 처리, 출력을 외부 DB 에 기록 등

=== 하둡과 분산 DB 비교

기존에도 MPP 가 있었지만 - 장비 클러스터에서 분석 SQL 쿼리를 병렬 수행하는 것에 초점 +
맵리듀스 + 분산 파일 시스템 - 아무 프로그램이나 실행 가능

* 저장소의 다양성
* 처리 모델의 다양성
* 결함을 줄이는 설계 - 결함을 다루는 방식, 메모리/디스크를 사용하는 방식
** MPP 는 장비 하나만 죽어도 전체 중단, 맵리듀스는 전체로는 영향을 받지 않음
** 맵리듀스는 되도록 디스크에 기록하려 한다 - 내결함성 확보 & 메모리에 올리기에는 너무 크다

(선점 내용은 뭐지)

== 맵리듀스를 넘어

=== Intermediate state materialization

한 작업에서 다른 작업으로 데이터를 옮기는 중간 상태 +
중간 상태를 파일로 기록하는 과정을 구체화 (미리 특정 연산을 만들어둔다)

유닉스 파이프에 비해서 중간 상태를 구체화하는 맵리듀스 접근법의 단점들

. 입력을 생성하는 모든 선행 작업이 완료됐을 때만 가능
. 매퍼가 중복될 수 있음
. 중간 상태 파일들이 여러 장비에 복제 - 임시로 쓰이기엔 과잉 조치

==== dataflow engine

위의 문제를 해결하기 위해 Spark, Tez, Flink 등의 연산 엔진이 나옴 +
전체 워크플로우를 하위 작업으로 나누지 않고 작업 하나로 다룸

데이터 흐름을 명시적으로 모델링 +
단일 스레드에서 사용자 정의 함수를 반복 호출, 입력을 파티셔닝하여 병렬화, 출력 -> 입력을 네트워크를 통해 복사

opearator: 유연한 함수 조합

맵리듀스 모델에 비한 장점

. 비싼 작업은 실제 필요할 때만 수행
. 필요없는 맵 태스트가 없음
. 워크플로우에 조인과 데이터 의존 관계를 명시적 선언 -> 지역성 최적화 가능
. 중간 상태는 메모리나 로컬 디스크로 충분 - HDFS 에 기록할 때보다 I/O 가 적음
. 입력이 준비되면 즉시 실행 가능
. JVM 재활용 가능 - 맵리듀스에서는 각 태스크마다 JVM 구동

==== 내결함성

중간 상태를 잃으면 유효한 데이터에서 계산을 다시 해서 복구

주어진 데이터가 어떻게 연산되는지 추적 필요

연산이 결정적인지 아닌지를 파악해야 한다 - 같은 출력을 생산하는가? +
비결정적인 경우 downstream 연산자도 죽이고 신규 데이터를 기준으로 다시 수행

연산자는 결정적으로 만드는게 좋다

==== 구체화에 대해

데이터플로 엔진은 유닉스 파이프와 비슷하다

정렬 연산자는 일시적으로라도 상태를 누적할 필요가 있다

작업을 완료하면 분산 파일 시스템에 다시 기록 - 보통 HDFS 상에 구체화된건 입력과 최종 출력

=== 그래프와 반복 처리

그래프는 분산 파일 시스템에 정점 vertex 과 간선 edge 목록이 포한된 파일 형태로 저장할 수 있다

대부분 반복적 스타일로 구현되는데, 맵리듀스에서는 반복적 속성을 고려하지 않기 때문에 비효율적

==== Pregel 처리 모델

build synchronous parallel (BSP) - Apache Giraph, Spark Grap X, Flink Gelly API

한 정점이 다른 정점으로 메시지를 보냄 +
반복할 때마다 개별 정점에서 함수 호출 - 보내진 모든 메시지를 전달 +
프리글 모델은 사용한 메모리 상태를 기억하고 있다

==== 내결함성

전체 상태를 지속성 있는 저장소에 기록하여 좋음

==== 병렬 실행

네트워크 상의 라우팅은 프리글 프레임워크가 담당한다

그래프 알고리즘은 장비 간 통신 오버헤드가 많이 발생 - 단일 장비에 넣을 수 있는 크기라면 단일 장비 알고리즘이 좋을 수도

그래프가 너무 크다면 프리글 고려

=== 고수준 API 와 언어

==== 선언형 질의 언어로 전환

관계형 연산자로 조인을 나타내면 프레임워크가 분석하여 자동으로 최적의 알고리즘을 결정

애플리케이션에서 선언적인 방법으로 어떤 조인이 필요한지 지정

고수준 API 에 선언적인 부분을 포함 & 쿼리 최적화하면 MPP 와 비슷 +
추가로 확장성